{"cells":[{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["import torch\n","from torchtext.legacy.data import Field, BucketIterator\n","from torchtext.legacy.datasets import SequenceTaggingDataset\n","from transformers import BertTokenizer\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertTokenizerFast, BertConfig, BertForTokenClassification\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["class AspectExtractionCorpus:\n","\n","\tdef __init__(self, model, input_directory, device, model_name):\n","\t\t\n","\t\tself.input_directory = input_directory\n","\t\tself.device = device\n","\t\tself.model = model\n","\n","\t\t# List all the fields.\n","\t\tif self.model in [\"muril\", \"mbert\"]:\n","\t\t\tself.bert_tokenizer = BertTokenizer.from_pretrained(model_name)\n","\t\t\tself.PAD_INDEX = self.bert_tokenizer.pad_token\n","\t\t\tself.CLS_INDEX = self.bert_tokenizer.cls_token\n","\t\t\tself.UNK_INDEX = self.bert_tokenizer.unk_token\n","\n","\t\t\tself.word_field = Field(batch_first = True,\n","\t\t\t\t\t\t\t\t\tsequential = False,\n","\t\t\t\t\t\t\t\t\tpad_token = self.PAD_INDEX,\n","\t\t\t\t\t\t\t\t\tinit_token = self.CLS_INDEX,\n","\t\t\t\t\t\t\t\t\tunk_token = self.UNK_INDEX)\n","\t\t\n","\t\telse:\n","\t\t\tself.word_field = Field(batch_first = True)\n","\n","\t\tself.tag_field = Field(unk_token = None, batch_first = True)\n","\t\tself.FIELDS = ((\"word\", self.word_field), (\"tag\", self.tag_field))\n","\n","\t\t# Create train and validation dataset using built-in parser from torchtext.\n","\t\tself.train_ds, self.val_ds = SequenceTaggingDataset.splits(path = input_directory, \n","\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ttrain = 'train.txt', \n","\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tvalidation = 'val.txt', \n","\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfields = self.FIELDS)\n","\n","\t\t# Convert fields to vocabulary list.\n","\t\tself.word_field.build_vocab(self.train_ds.word)   # ADD VECTORS HERE.\n","\t\tself.tag_field.build_vocab(self.train_ds.tag)\n","\n","\t\t# Prepare padding index to be ignored during model training/evaluation.\n","\t\tself.word_pad_idx = self.word_field.vocab.stoi[self.word_field.pad_token]\n","\t\tself.tag_pad_idx = self.tag_field.vocab.stoi[self.tag_field.pad_token]\n","\n","\t\t# Vocabulary and Tagset size.\n","\t\tself.vocab_size = len(self.word_field.vocab.itos)    # Includeds <pad> and <unk> as well.\n","\t\tself.tagset_size = len(self.tag_field.vocab.itos)    # Includes <pad> as well.\n","\n","\tdef print_statistics(self):\n","\t\t\"\"\"\n","\t\tPrints the data statistics.\n","\t\t\"\"\"\n","\t\tprint('\\nLocation of dataset : ', self.input_directory)\n","\t\tprint('Length of training dataset : ', len(self.train_ds))\n","\t\tprint('Length of validation dataset : ', len(self.val_ds))\n","\t\tprint('Length of text vocab (unique words in dataset) : ', self.vocab_size)\n","\t\tprint('Length of label vocab (unique tags in labels) : ', self.tagset_size)\n","\t\tprint()\n","\n","\tdef load_data(self, batch_size: int, shuffle: bool = False):\n","\t\t'''\n","\t\tGenerates the data iterators for train and validation data.\n","\n","\t\tParameters\n","\t\t----------\n","\t\tbatch_size : int\n","\t\t\tbatch_size.\n","\t\tshuffle : Bool, optional\n","\t\t\tWhether to shuffle the data before training/testing. The default is True.\n","\n","\t\tReturns\n","\t\t-------\n","\t\ttrain_iter : training Dataloader instance.\n","\t\tval_iter : validation Dataloader instance.\n","\t\t'''\n","\t\ttrain_dl, val_dl = BucketIterator.splits(datasets = (self.train_ds, self.val_ds),\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tbatch_sizes = (batch_size, batch_size),\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tshuffle = shuffle,\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tsort_key = lambda x: len(x.word),\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tsort_within_batch = True,\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\t\trepeat = False,\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdevice = self.device)\n","\n","\t\treturn train_dl, val_dl                                                      \n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["root = r'D:\\ML_projects\\IPV-Project\\data\\aspect_extraction\\kfold\\2'"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["from pathlib import Path\n","import re\n","\n","def flatten(mylist: list):\n","    return [item for sublist in mylist for item in sublist]\n","\n","def read_CoNLL(file_path):\n","    file_path = Path(file_path)\n","\n","    raw_text = file_path.read_text(encoding='utf8').strip()\n","    raw_docs = re.split(r'\\n\\t?\\n', raw_text)\n","    token_docs = []\n","    tag_docs = []\n","    for doc in raw_docs:\n","        tokens = []\n","        tags = []\n","        for line in doc.split('\\n'):\n","            token, tag = line.split('\\t')\n","            tokens.append(token)\n","            tags.append(tag)\n","        token_docs.append(tokens)\n","        tag_docs.append(tags)\n","\n","    return token_docs, tag_docs\n"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"data":{"text/plain":["{'B-Others',\n"," 'B-character_assasination',\n"," 'B-ethnic_violence',\n"," 'B-general_threat',\n"," 'B-physical_threat',\n"," 'B-profanity',\n"," 'B-rape_threat',\n"," 'B-religion_violence',\n"," 'B-sexism',\n"," 'I-Others',\n"," 'I-character_assasination',\n"," 'I-ethnic_violence',\n"," 'I-general_threat',\n"," 'I-physical_threat',\n"," 'I-profanity',\n"," 'I-rape_threat',\n"," 'I-religion_violence',\n"," 'I-sexism',\n"," 'O'}"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["texts, tags = read_CoNLL(os.path.join(root, 'train.txt'))\n","tags_list = set(flatten(tags))\n","tags_list"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"text/plain":["{'B-character_assasination': 0,\n"," 'I-Others': 1,\n"," 'B-rape_threat': 2,\n"," 'B-profanity': 3,\n"," 'B-Others': 4,\n"," 'I-rape_threat': 5,\n"," 'I-profanity': 6,\n"," 'B-general_threat': 7,\n"," 'B-religion_violence': 8,\n"," 'B-physical_threat': 9,\n"," 'I-character_assasination': 10,\n"," 'I-religion_violence': 11,\n"," 'I-general_threat': 12,\n"," 'I-sexism': 13,\n"," 'I-physical_threat': 14,\n"," 'B-sexism': 15,\n"," 'O': 16,\n"," 'I-ethnic_violence': 17,\n"," 'B-ethnic_violence': 18}"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["labels_to_ids = {k: v for v, k in enumerate(tags_list)}\n","ids_to_labels = {v: k for v, k in enumerate(tags_list)}\n","\n","labels_to_ids"]},{"cell_type":"code","execution_count":90,"metadata":{},"outputs":[],"source":["class AspectDataset(Dataset):\n","  def __init__(self, input_directory, tokenizer, max_len, labels_to_ids = None, is_train = True):\n","        self.input_directory = input_directory\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","        self.sentences, self.labels = read_CoNLL(self.input_directory)\n","        self.tags_list = set(flatten(self.labels))\n","\n","        if is_train:\n","              self.labels_to_ids = {k: v for v, k in enumerate(self.tags_list)}\n","              self.ids_to_labels = {v: k for v, k in enumerate(self.tags_list)}\n","        else:\n","              self.labels_to_ids = labels_to_ids\n","              self.ids_to_labels = {v : k for k, v in self.labels_to_ids.items()}\n","\n","\n","  def __getitem__(self, index):\n","\n","        # step 1: get the sentence and word labels \n","        sentence = self.sentences[index]  \n","        word_labels = self.labels[index] \n","\n","        # step 2: use tokenizer to encode sentence (includes padding/truncation up to max length)\n","        # BertTokenizerFast provides a handy \"return_offsets_mapping\" functionality for individual tokens\n","        encoding = self.tokenizer(sentence,\n","                                  is_split_into_words=True, \n","                                  return_offsets_mapping=True, \n","                                  padding='max_length', \n","                                  truncation=True, \n","                                  max_length=self.max_len)\n","        \n","        # step 3: create token labels only for first word pieces of each tokenized word\n","        labels = [self.labels_to_ids[label] for label in word_labels] \n","        \n","        # code based on https://huggingface.co/transformers/custom_datasets.html#tok-ner\n","        # create an empty array of -100 of length max_length\n","        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n","        \n","        # set only labels whose first offset position is 0 and the second is not 0\n","        i = 0\n","        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n","          if mapping[0] == 0 and mapping[1] != 0:\n","            # overwrite label\n","            encoded_labels[idx] = labels[i]\n","            i += 1\n","\n","        # step 4: turn everything into PyTorch tensors\n","        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n","        item['labels'] = torch.as_tensor(encoded_labels)\n","\n","        # Get lengths.\n","        item['seq_length'] = sum(encoding['attention_mask'])\n","        \n","        return item\n","\n","  def __len__(self):\n","        return len(self.sentences)"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["tokenizer = BertTokenizerFast.from_pretrained('google/muril-base-cased')"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[{"data":{"text/plain":["'D:\\\\ML_projects\\\\IPV-Project\\\\data\\\\aspect_extraction\\\\kfold\\\\2'"]},"execution_count":70,"metadata":{},"output_type":"execute_result"}],"source":["MAX_LEN = 128\n","root"]},{"cell_type":"code","execution_count":91,"metadata":{},"outputs":[],"source":["train_ds = AspectDataset(os.path.join(root, \"train.txt\"), tokenizer, MAX_LEN)\n","labels_to_ids = train_ds.labels_to_ids\n","val_ds = AspectDataset(os.path.join(root, \"val.txt\"), tokenizer, MAX_LEN, labels_to_ids, is_train=False)"]},{"cell_type":"code","execution_count":108,"metadata":{},"outputs":[],"source":["train_dl = DataLoader(train_ds, batch_size=8, shuffle=False)"]},{"cell_type":"code","execution_count":97,"metadata":{},"outputs":[],"source":["train_dl, val_dl = BucketIterator.splits((train_ds, val_ds), batch_sizes=(8,8), sort = False, sort_within_batch = True, sort_key = lambda x: x['seq_length'])\n"]},{"cell_type":"code","execution_count":109,"metadata":{},"outputs":[{"data":{"text/plain":["{'input_ids': tensor([[   104,   1386,   8494,  ...,      0,      0,      0],\n","         [   104,  20643,   6370,  ...,      0,      0,      0],\n","         [   104,   7741,  73689,  ...,      0,      0,      0],\n","         ...,\n","         [   104, 196970, 168705,  ...,      0,      0,      0],\n","         [   104,  16316, 163572,  ...,      0,      0,      0],\n","         [   104,    446,   4277,  ...,      0,      0,      0]]),\n"," 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n","         [0, 0, 0,  ..., 0, 0, 0],\n","         [0, 0, 0,  ..., 0, 0, 0],\n","         ...,\n","         [0, 0, 0,  ..., 0, 0, 0],\n","         [0, 0, 0,  ..., 0, 0, 0],\n","         [0, 0, 0,  ..., 0, 0, 0]]),\n"," 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n","         [1, 1, 1,  ..., 0, 0, 0],\n","         [1, 1, 1,  ..., 0, 0, 0],\n","         ...,\n","         [1, 1, 1,  ..., 0, 0, 0],\n","         [1, 1, 1,  ..., 0, 0, 0],\n","         [1, 1, 1,  ..., 0, 0, 0]]),\n"," 'offset_mapping': tensor([[[ 0,  0],\n","          [ 1,  3],\n","          [ 3,  5],\n","          ...,\n","          [ 0,  0],\n","          [ 0,  0],\n","          [ 0,  0]],\n"," \n","         [[ 0,  0],\n","          [ 0,  8],\n","          [ 8, 10],\n","          ...,\n","          [ 0,  0],\n","          [ 0,  0],\n","          [ 0,  0]],\n"," \n","         [[ 0,  0],\n","          [ 0,  6],\n","          [ 6,  9],\n","          ...,\n","          [ 0,  0],\n","          [ 0,  0],\n","          [ 0,  0]],\n"," \n","         ...,\n"," \n","         [[ 0,  0],\n","          [ 0,  2],\n","          [ 2,  5],\n","          ...,\n","          [ 0,  0],\n","          [ 0,  0],\n","          [ 0,  0]],\n"," \n","         [[ 0,  0],\n","          [ 0,  4],\n","          [ 0,  4],\n","          ...,\n","          [ 0,  0],\n","          [ 0,  0],\n","          [ 0,  0]],\n"," \n","         [[ 0,  0],\n","          [ 0,  1],\n","          [ 1,  2],\n","          ...,\n","          [ 0,  0],\n","          [ 0,  0],\n","          [ 0,  0]]]),\n"," 'labels': tensor([[-100, -100, -100,  ..., -100, -100, -100],\n","         [-100,    0, -100,  ..., -100, -100, -100],\n","         [-100,    0, -100,  ..., -100, -100, -100],\n","         ...,\n","         [-100,   16, -100,  ..., -100, -100, -100],\n","         [-100,    3,    6,  ..., -100, -100, -100],\n","         [-100,   16, -100,  ..., -100, -100, -100]], dtype=torch.int32),\n"," 'seq_length': tensor([15, 12, 21, 14, 39, 31, 26, 16])}"]},"execution_count":109,"metadata":{},"output_type":"execute_result"}],"source":["next(iter(train_dl))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"interpreter":{"hash":"63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"},"kernelspec":{"display_name":"Python 3.9.5 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
