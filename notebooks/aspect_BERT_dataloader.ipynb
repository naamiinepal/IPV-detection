{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "from torchtext.legacy.datasets import SequenceTaggingDataset\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertConfig, BertForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AspectExtractionCorpus:\n",
    "    def __init__(self, model, input_directory, device, model_name):\n",
    "\n",
    "        self.input_directory = input_directory\n",
    "        self.device = device\n",
    "        self.model = model\n",
    "\n",
    "        # List all the fields.\n",
    "        if self.model in [\"muril\", \"mbert\"]:\n",
    "            self.bert_tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "            self.PAD_INDEX = self.bert_tokenizer.pad_token\n",
    "            self.CLS_INDEX = self.bert_tokenizer.cls_token\n",
    "            self.UNK_INDEX = self.bert_tokenizer.unk_token\n",
    "\n",
    "            self.word_field = Field(\n",
    "                batch_first=True,\n",
    "                sequential=False,\n",
    "                pad_token=self.PAD_INDEX,\n",
    "                init_token=self.CLS_INDEX,\n",
    "                unk_token=self.UNK_INDEX,\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self.word_field = Field(batch_first=True)\n",
    "\n",
    "        self.tag_field = Field(unk_token=None, batch_first=True)\n",
    "        self.FIELDS = ((\"word\", self.word_field), (\"tag\", self.tag_field))\n",
    "\n",
    "        # Create train and validation dataset using built-in parser from torchtext.\n",
    "        self.train_ds, self.val_ds = SequenceTaggingDataset.splits(\n",
    "            path=input_directory,\n",
    "            train=\"train.txt\",\n",
    "            validation=\"val.txt\",\n",
    "            fields=self.FIELDS,\n",
    "        )\n",
    "\n",
    "        # Convert fields to vocabulary list.\n",
    "        self.word_field.build_vocab(self.train_ds.word)  # ADD VECTORS HERE.\n",
    "        self.tag_field.build_vocab(self.train_ds.tag)\n",
    "\n",
    "        # Prepare padding index to be ignored during model training/evaluation.\n",
    "        self.word_pad_idx = self.word_field.vocab.stoi[self.word_field.pad_token]\n",
    "        self.tag_pad_idx = self.tag_field.vocab.stoi[self.tag_field.pad_token]\n",
    "\n",
    "        # Vocabulary and Tagset size.\n",
    "        self.vocab_size = len(\n",
    "            self.word_field.vocab.itos\n",
    "        )  # Includeds <pad> and <unk> as well.\n",
    "        self.tagset_size = len(self.tag_field.vocab.itos)  # Includes <pad> as well.\n",
    "\n",
    "    def print_statistics(self):\n",
    "        \"\"\"\n",
    "        Prints the data statistics.\n",
    "        \"\"\"\n",
    "        print(\"\\nLocation of dataset : \", self.input_directory)\n",
    "        print(\"Length of training dataset : \", len(self.train_ds))\n",
    "        print(\"Length of validation dataset : \", len(self.val_ds))\n",
    "        print(\"Length of text vocab (unique words in dataset) : \", self.vocab_size)\n",
    "        print(\"Length of label vocab (unique tags in labels) : \", self.tagset_size)\n",
    "        print()\n",
    "\n",
    "    def load_data(self, batch_size: int, shuffle: bool = False):\n",
    "        \"\"\"\n",
    "        Generates the data iterators for train and validation data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size : int\n",
    "                batch_size.\n",
    "        shuffle : Bool, optional\n",
    "                Whether to shuffle the data before training/testing. The default is True.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        train_iter : training Dataloader instance.\n",
    "        val_iter : validation Dataloader instance.\n",
    "        \"\"\"\n",
    "        train_dl, val_dl = BucketIterator.splits(\n",
    "            datasets=(self.train_ds, self.val_ds),\n",
    "            batch_sizes=(batch_size, batch_size),\n",
    "            shuffle=shuffle,\n",
    "            sort_key=lambda x: len(x.word),\n",
    "            sort_within_batch=True,\n",
    "            repeat=False,\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "        return train_dl, val_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = r\"D:\\ML_projects\\IPV-Project\\data\\aspect_extraction\\kfold\\2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "\n",
    "def flatten(mylist: list):\n",
    "    return [item for sublist in mylist for item in sublist]\n",
    "\n",
    "\n",
    "def read_CoNLL(file_path):\n",
    "    file_path = Path(file_path)\n",
    "\n",
    "    raw_text = file_path.read_text(encoding=\"utf8\").strip()\n",
    "    raw_docs = re.split(r\"\\n\\t?\\n\", raw_text)\n",
    "    token_docs = []\n",
    "    tag_docs = []\n",
    "    for doc in raw_docs:\n",
    "        tokens = []\n",
    "        tags = []\n",
    "        for line in doc.split(\"\\n\"):\n",
    "            token, tag = line.split(\"\\t\")\n",
    "            tokens.append(token)\n",
    "            tags.append(tag)\n",
    "        token_docs.append(tokens)\n",
    "        tag_docs.append(tags)\n",
    "\n",
    "    return token_docs, tag_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-Others',\n",
       " 'B-character_assasination',\n",
       " 'B-ethnic_violence',\n",
       " 'B-general_threat',\n",
       " 'B-physical_threat',\n",
       " 'B-profanity',\n",
       " 'B-rape_threat',\n",
       " 'B-religion_violence',\n",
       " 'B-sexism',\n",
       " 'I-Others',\n",
       " 'I-character_assasination',\n",
       " 'I-ethnic_violence',\n",
       " 'I-general_threat',\n",
       " 'I-physical_threat',\n",
       " 'I-profanity',\n",
       " 'I-rape_threat',\n",
       " 'I-religion_violence',\n",
       " 'I-sexism',\n",
       " 'O'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts, tags = read_CoNLL(os.path.join(root, \"train.txt\"))\n",
    "tags_list = set(flatten(tags))\n",
    "tags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-character_assasination': 0,\n",
       " 'I-Others': 1,\n",
       " 'B-rape_threat': 2,\n",
       " 'B-profanity': 3,\n",
       " 'B-Others': 4,\n",
       " 'I-rape_threat': 5,\n",
       " 'I-profanity': 6,\n",
       " 'B-general_threat': 7,\n",
       " 'B-religion_violence': 8,\n",
       " 'B-physical_threat': 9,\n",
       " 'I-character_assasination': 10,\n",
       " 'I-religion_violence': 11,\n",
       " 'I-general_threat': 12,\n",
       " 'I-sexism': 13,\n",
       " 'I-physical_threat': 14,\n",
       " 'B-sexism': 15,\n",
       " 'O': 16,\n",
       " 'I-ethnic_violence': 17,\n",
       " 'B-ethnic_violence': 18}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_to_ids = {k: v for v, k in enumerate(tags_list)}\n",
    "ids_to_labels = {v: k for v, k in enumerate(tags_list)}\n",
    "\n",
    "labels_to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AspectDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, input_directory, tokenizer, max_len, labels_to_ids=None, is_train=True\n",
    "    ):\n",
    "        self.input_directory = input_directory\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.sentences, self.labels = read_CoNLL(self.input_directory)\n",
    "        self.tags_list = set(flatten(self.labels))\n",
    "\n",
    "        if is_train:\n",
    "            self.labels_to_ids = {k: v for v, k in enumerate(self.tags_list)}\n",
    "            self.ids_to_labels = {v: k for v, k in enumerate(self.tags_list)}\n",
    "        else:\n",
    "            self.labels_to_ids = labels_to_ids\n",
    "            self.ids_to_labels = {v: k for k, v in self.labels_to_ids.items()}\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # step 1: get the sentence and word labels\n",
    "        sentence = self.sentences[index]\n",
    "        word_labels = self.labels[index]\n",
    "\n",
    "        # step 2: use tokenizer to encode sentence (includes padding/truncation up to max length)\n",
    "        # BertTokenizerFast provides a handy \"return_offsets_mapping\" functionality for individual tokens\n",
    "        encoding = self.tokenizer(\n",
    "            sentence,\n",
    "            is_split_into_words=True,\n",
    "            return_offsets_mapping=True,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "        )\n",
    "\n",
    "        # step 3: create token labels only for first word pieces of each tokenized word\n",
    "        labels = [self.labels_to_ids[label] for label in word_labels]\n",
    "\n",
    "        # code based on https://huggingface.co/transformers/custom_datasets.html#tok-ner\n",
    "        # create an empty array of -100 of length max_length\n",
    "        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n",
    "\n",
    "        # set only labels whose first offset position is 0 and the second is not 0\n",
    "        i = 0\n",
    "        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n",
    "            if mapping[0] == 0 and mapping[1] != 0:\n",
    "                # overwrite label\n",
    "                encoded_labels[idx] = labels[i]\n",
    "                i += 1\n",
    "\n",
    "        # step 4: turn everything into PyTorch tensors\n",
    "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "        item[\"labels\"] = torch.as_tensor(encoded_labels)\n",
    "\n",
    "        # Get lengths.\n",
    "        item[\"seq_length\"] = sum(encoding[\"attention_mask\"])\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"google/muril-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\ML_projects\\\\IPV-Project\\\\data\\\\aspect_extraction\\\\kfold\\\\2'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = 128\n",
    "root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = AspectDataset(os.path.join(root, \"train.txt\"), tokenizer, MAX_LEN)\n",
    "labels_to_ids = train_ds.labels_to_ids\n",
    "val_ds = AspectDataset(\n",
    "    os.path.join(root, \"val.txt\"), tokenizer, MAX_LEN, labels_to_ids, is_train=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, val_dl = BucketIterator.splits(\n",
    "    (train_ds, val_ds),\n",
    "    batch_sizes=(8, 8),\n",
    "    sort=False,\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x: x[\"seq_length\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   104,   1386,   8494,  ...,      0,      0,      0],\n",
       "         [   104,  20643,   6370,  ...,      0,      0,      0],\n",
       "         [   104,   7741,  73689,  ...,      0,      0,      0],\n",
       "         ...,\n",
       "         [   104, 196970, 168705,  ...,      0,      0,      0],\n",
       "         [   104,  16316, 163572,  ...,      0,      0,      0],\n",
       "         [   104,    446,   4277,  ...,      0,      0,      0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'offset_mapping': tensor([[[ 0,  0],\n",
       "          [ 1,  3],\n",
       "          [ 3,  5],\n",
       "          ...,\n",
       "          [ 0,  0],\n",
       "          [ 0,  0],\n",
       "          [ 0,  0]],\n",
       " \n",
       "         [[ 0,  0],\n",
       "          [ 0,  8],\n",
       "          [ 8, 10],\n",
       "          ...,\n",
       "          [ 0,  0],\n",
       "          [ 0,  0],\n",
       "          [ 0,  0]],\n",
       " \n",
       "         [[ 0,  0],\n",
       "          [ 0,  6],\n",
       "          [ 6,  9],\n",
       "          ...,\n",
       "          [ 0,  0],\n",
       "          [ 0,  0],\n",
       "          [ 0,  0]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0,  0],\n",
       "          [ 0,  2],\n",
       "          [ 2,  5],\n",
       "          ...,\n",
       "          [ 0,  0],\n",
       "          [ 0,  0],\n",
       "          [ 0,  0]],\n",
       " \n",
       "         [[ 0,  0],\n",
       "          [ 0,  4],\n",
       "          [ 0,  4],\n",
       "          ...,\n",
       "          [ 0,  0],\n",
       "          [ 0,  0],\n",
       "          [ 0,  0]],\n",
       " \n",
       "         [[ 0,  0],\n",
       "          [ 0,  1],\n",
       "          [ 1,  2],\n",
       "          ...,\n",
       "          [ 0,  0],\n",
       "          [ 0,  0],\n",
       "          [ 0,  0]]]),\n",
       " 'labels': tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
       "         [-100,    0, -100,  ..., -100, -100, -100],\n",
       "         [-100,    0, -100,  ..., -100, -100, -100],\n",
       "         ...,\n",
       "         [-100,   16, -100,  ..., -100, -100, -100],\n",
       "         [-100,    3,    6,  ..., -100, -100, -100],\n",
       "         [-100,   16, -100,  ..., -100, -100, -100]], dtype=torch.int32),\n",
       " 'seq_length': tensor([15, 12, 21, 14, 39, 31, 26, 16])}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
