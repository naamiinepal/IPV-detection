# pytorch_lightning==1.8.6
seed_everything: 4242
trainer:
  logger:
      - class_path: pytorch_lightning.loggers.WandbLogger
        init_args:
            project: aspect_detection
            name: word_muril_fold1
  callbacks:
    - class_path: pytorch_lightning.callbacks.ModelCheckpoint
      init_args:
        save_top_k: 10
        monitor: ${model.init_args.monitor}
        mode: min
        dirpath: checkpoints/word_muril_fold1
        save_weights_only: True
        filename: "{epoch:02d}-{val_loss:.3f}-{overall_f1:.3f}"
    - class_path: pytorch_lightning.callbacks.LearningRateMonitor
    - class_path: pytorch_lightning.callbacks.EarlyStopping
      init_args:
        monitor: ${model.init_args.monitor}
        patience: 10
        mode: min
        verbose: True
  devices: [0]
  max_epochs: 60
  accelerator: gpu
  log_every_n_steps: 33
  # fast_dev_run: True
ckpt_path: null
model:
  class_path: models.word_model.WordModel
  init_args:
    model_name_or_path: ${data.init_args.model_name_or_path}
    dropout_rate: 0.1
    learning_rate: 5.0e-05
    weight_decay: 0.01
    plateu_factor: 0.2
    plateu_patience: 4
    monitor: val_loss
data:
  class_path: datamodules.word_datamodule.WordDataModule
  init_args:
    dataset_path: datasets/word
    current_fold: 1
    model_name_or_path: google/muril-base-cased
    batch_size: 32
    max_workers: 4
    pin_memory: true
